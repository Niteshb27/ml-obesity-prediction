# Loading necessary libraries
install.packages("tidyverse")
install.packages("conflicted")
library(conflicted)
library(tidyverse)
library(lattice)
library(caret)

#Getting the data
OD <- read.csv('OD.csv')

# Defining the mode_impute function
mode_impute <- function(x) {
  mode_val <- as.character(x[which.max(tabulate(match(x, unique(x))))])
  x[is.na(x)] <- mode_val
  return(x)
}

# Defining a function to calculate outlier bounds using IQR
calculate_iqr_bounds <- function(column) {
  Q1 <- quantile(column, 0.25, na.rm = TRUE)
  Q3 <- quantile(column, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  return(list(lower = lower_bound, upper = upper_bound))
}

# Applying outlier detection and removal for all numeric columns
data_cleaned <- OD %>%
  
  # Imputing missing values and clean data types
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .))) %>%
  mutate(across(where(is.character), mode_impute)) %>%
  distinct() %>%
  mutate(
    Gender = as.factor(Gender),
    Age = as.numeric(Age),
    Height = as.numeric(Height),
    Weight = as.numeric(Weight),
    family_history_with_overweight = as.factor(family_history_with_overweight),
    FAVC = as.factor(FAVC),
    FCVC = as.numeric(FCVC),
    NCP = as.numeric(NCP),
    CAEC = as.factor(CAEC),
    SMOKE = as.factor(SMOKE),
    CH2O = as.numeric(CH2O),
    SCC = as.factor(SCC),
    FAF = as.numeric(FAF),
    TUE = as.numeric(TUE),
    CALC = as.factor(CALC),
    MTRANS = as.factor(MTRANS),
    Nobeyesdad = as.factor(Nobeyesdad)
  ) %>%
  
  # Calculating IQR bounds and removing outliers
  mutate(across(where(is.numeric), ~ {
    bounds <- calculate_iqr_bounds(.)
    if (!is.null(bounds)) {
      # Replacing outlier values with NA
      .[. < bounds$lower | . > bounds$upper] <- NA
    }
    .
  })) %>%
  # Removing rows with NA values generated by outlier removal
  drop_na()

# Displaying the cleaned data
print(head(data_cleaned))



# Defining Standardization function
standardize <- function(x) {
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}

# Applying Standardization to numeric columns
data_standardized <- data_cleaned %>%
  mutate(across(where(is.numeric), standardize))

# Displaying the first few rows of the standardized dataset
print(head(data_standardized))


# Getting the structure of the data
str(data_standardized)

# Getting the dimensions of the data
dim(data_standardized)

# Getting the column names of the data
colnames(data_standardized)

# Summary statistics for numerical variables
summary(select_if(data_standardized, is.numeric))

# Summary statistics for categorical variables
summary(select_if(data_standardized, is.factor))


# Histogram for numerical variables
data_standardized %>% 
  select_if(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
  geom_histogram(bins = 30) + 
  facet_wrap(~key, scales = 'free_x')

# Boxplot for numerical variables
data_standardized %>% 
  select_if(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(key, value)) + 
  geom_boxplot() + 
  coord_flip()

# Bar plot for categorical variables
data_standardized%>% 
  select_if(is.factor) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
  geom_bar() + 
  facet_wrap(~key, scales = 'free_x')

# Scatter plot for pairwise relationships
install.packages("GGally")
library(GGally)
ggpairs(select_if(data_standardized, is.numeric)) 

# Correlation matrix
cor_matrix <- cor(select_if(data_standardized, is.numeric), use = "complete.obs")

# Visualizing correlation matrix
library(corrplot)
corrplot(cor_matrix, method = "circle")

# Distribution plots for numerical variables
data_standardized %>% 
  select_if(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
  geom_density() + 
  facet_wrap(~key, scales = 'free')

# Distribution plots for categorical variables
data_standardized %>% 
  select_if(is.factor) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
  geom_bar() + 
  facet_wrap(~key, scales = 'free')

# Set seed for reproducibility
set.seed(123)

# Splitting the data into training and testing sets (80% training, 20% testing)
train_index <- createDataPartition(data_standardized$Nobeyesdad, p = 0.8, list = FALSE)
train_data <- data_standardized[train_index, ]
test_data <- data_standardized[-train_index, ]

# Displaying the dimensions of the training and testing sets
dim(train_data)
dim(test_data)

# Visualizing the distribution of the target variable in the training set
ggplot(train_data, aes(x = Nobeyesdad)) +
  geom_bar() +
  ggtitle("Distribution of Target Variable in Training Set")

# Visualizing the distribution of the target variable in the testing set
ggplot(test_data, aes(x = Nobeyesdad)) +
  geom_bar() +
  ggtitle("Distribution of Target Variable in Testing Set")

# Checking the distribution of the target variable
table(data_standardized$Nobeyesdad)

#installing and loading needful packages and libraries
install.packages("caret")
install.packages("nnet")
install.packages("randomForest")
install.packages("rpart")
install.packages("e1071")
install.packages("conflicted")
library(conflicted)
library(tidyverse)
library(caret)
library(nnet)
library(randomForest)
library(rpart)
library(e1071)

# Verifying the column names
print(colnames(data_standardized))

# Checking the structure of the data
print(str(data_standardized))

# Checking for missing values in the target column
print(sum(is.na(data_standardized$Nobeyesdad)))

# Displaying unique values in the target column to ensure they are valid
print(unique(data_standardized$Nobeyesdad))

# Displaying the first few rows of the data
print(head(data_standardized))


# Training multi-nominal logistic regression model

# Define trainControl
ctrl <- trainControl(
  method = "cv",            # Cross-validation
  number = 5,               # Number of folds
  verboseIter = TRUE        # Show progress
)
multinom_model <- train(Nobeyesdad ~ ., data = train_data, method = "multinom", trControl = ctrl)

log_pred <- predict(multinom_model, newdata = test_data)

# Converting to factors
log_pred <- as.factor(log_pred)
test_data$Nobeyesdad <- as.factor(test_data$Nobeyesdad)

# Ensuring that the levels are the same
log_pred <- factor(log_pred, levels = levels(test_data$Nobeyesdad))


library(MLmetrics)

# Calculating precision, recall, and F1 for each class
unique_classes1 <- levels(test_data$Nobeyesdad)

# Initializing vectors to store results
precision_scores <- c()
recall_scores <- c()
f1_scores <- c()

# Loop over each class
for (class_name1 in unique_classes1) {
  precision <- Precision(test_data$Nobeyesdad, log_pred, positive = class_name1)
  recall <- Recall(test_data$Nobeyesdad, log_pred, positive = class_name1)
  f1 <- F1_Score(test_data$Nobeyesdad, log_pred, positive = class_name1)
  
  # Storing the results
  precision_scores <- c(precision_scores, precision)
  recall_scores <- c(recall_scores, recall)
  f1_scores <- c(f1_scores, f1)
}

# Combining results into a data frame
per_class_results1 <- data.frame(Class = unique_classes1, Precision = precision_scores, Recall = recall_scores, F1_Score = f1_scores)

#Generating confusion matrix
log_cm <- confusionMatrix(log_pred, test_data$Nobeyesdad)

#Combining and displaying the metrics and confusion matrix
cat("\nConfusion Matrix:\n")
print(log_cm$table)

cat("\nPer-class Metrics:\n")
print(per_class_results1)

cat("\nDetailed Confusion Matrix:\n")
print(log_cm)



# Training decision tree model
tree_model <- train(Nobeyesdad ~ ., data = train_data, method = "rpart")

tree_pred <- predict(tree_model, newdata = test_data)

tree_pred <- as.factor(tree_pred)
test_data$Nobeyesdad <- as.factor(test_data$Nobeyesdad)
tree_pred <- factor(tree_pred, levels = levels(test_data$Nobeyesdad))

#Calculating metrics for each class
unique_classes2 <- levels(test_data$Nobeyesdad)

# Initializing vectors to store results
precision_scores <- c()
recall_scores <- c()
f1_scores <- c()

# Loop over each class to calculate Precision, Recall, and F1 Score
for (class_name2 in unique_classes2) {
  precision <- Precision(test_data$Nobeyesdad, tree_pred, positive = class_name2)
  recall <- Recall(test_data$Nobeyesdad, tree_pred, positive = class_name2)
  f1 <- F1_Score(test_data$Nobeyesdad, tree_pred, positive = class_name2)
  
  # Storing the results
  precision_scores <- c(precision_scores, precision)
  recall_scores <- c(recall_scores, recall)
  f1_scores <- c(f1_scores, f1)
}
# Combine results into a data frame for per-class metrics
per_class_results2 <- data.frame(Class = unique_classes2, Precision = precision_scores, Recall = recall_scores, F1_Score = f1_scores)

tree_cm <- confusionMatrix(tree_pred, test_data$Nobeyesdad)

#Combining and displaying the metrics and confusion matrix
cat("\nConfusion Matrix:\n")
print(tree_cm$table)

cat("\nPer-class Metrics:\n")
print(per_class_results2)

cat("\nDetailed Confusion Matrix:\n")
print(tree_cm)


# Training random forest model
rf_model <- train(Nobeyesdad ~ ., data = train_data, method = "rf", importance = TRUE)

rf_pred <- predict(rf_model, newdata = test_data)

rf_pred <- as.factor(rf_pred)
test_data$Nobeyesdad <- as.factor(test_data$Nobeyesdad)
rf_pred <- factor(rf_pred, levels = levels(test_data$Nobeyesdad))

unique_classes3 <- levels(test_data$Nobeyesdad)

# Initializing vectors to store results
precision_scores <- c()
recall_scores <- c()
f1_scores <- c()

# Loop over each class to calculate Precision, Recall, and F1 Score
for (class_name3 in unique_classes3) {
  precision <- Precision(test_data$Nobeyesdad, rf_pred, positive = class_name3)
  recall <- Recall(test_data$Nobeyesdad, rf_pred, positive = class_name3)
  f1 <- F1_Score(test_data$Nobeyesdad, rf_pred, positive = class_name3)
  
  # Storing the results
  precision_scores <- c(precision_scores, precision)
  recall_scores <- c(recall_scores, recall)
  f1_scores <- c(f1_scores, f1)
}

# Combining results into a data frame for per-class metrics
per_class_results3 <- data.frame(Class = unique_classes3, Precision = precision_scores, Recall = recall_scores, F1_Score = f1_scores)

rf_cm <- confusionMatrix(rf_pred, test_data$Nobeyesdad)

#Combining and displaying the metrics and confusion matrix
cat("\nConfusion Matrix:\n")
print(rf_cm$table)

cat("\nPer-class Metrics:\n")
print(per_class_results3)

cat("\nDetailed Confusion Matrix:\n")
print(rf_cm)


# Training SVM model
svm_model <- train(Nobeyesdad ~ ., data = train_data, method = "svmLinear")

svm_pred <- predict(svm_model, newdata = test_data)

svm_pred <- as.factor(svm_pred)
test_data$Nobeyesdad <- as.factor(test_data$Nobeyesdad)
svm_pred <- factor(svm_pred, levels = levels(test_data$Nobeyesdad))

unique_classes4 <- levels(test_data$Nobeyesdad)

# Initializing vectors to store results
precision_scores <- c()
recall_scores <- c()
f1_scores <- c()

# Loop over each class to calculate Precision, Recall, and F1 Score
for (class_name4 in unique_classes4) {
  precision <- Precision(test_data$Nobeyesdad, svm_pred, positive = class_name4)
  recall <- Recall(test_data$Nobeyesdad, svm_pred, positive = class_name4)
  f1 <- F1_Score(test_data$Nobeyesdad, svm_pred, positive = class_name4)
  
  # Storing the results
  precision_scores <- c(precision_scores, precision)
  recall_scores <- c(recall_scores, recall)
  f1_scores <- c(f1_scores, f1)
}

# Combining results into a data frame for per-class metrics
per_class_results4 <- data.frame(Class = unique_classes4, Precision = precision_scores, Recall = recall_scores, F1_Score = f1_scores)

svm_cm <- confusionMatrix(svm_pred, test_data$Nobeyesdad)

#Combining and displaying the metrics and confusion matrix
cat("\nConfusion Matrix:\n")
print(svm_cm$table)

cat("\nPer-class Metrics:\n")
print(per_class_results4)

cat("\nDetailed Confusion Matrix:\n")
print(svm_cm)


# Extracting metrics from confusion matrices
log_metrics <- log_cm$overall
tree_metrics <- tree_cm$overall
rf_metrics <- rf_cm$overall
svm_metrics <- svm_cm$overall

# Printing metrics
print(log_metrics)
print(tree_metrics)
print(rf_metrics)
print(svm_metrics)


library(ggplot2)

# Combining metrics into a data frame
metrics_df <- data.frame(
  Model = c("Logistic Regression", "Decision Tree", "Random Forest", "SVM"),
  Accuracy = c(log_cm$overall["Accuracy"], tree_cm$overall["Accuracy"], rf_cm$overall["Accuracy"], svm_cm$overall["Accuracy"]),
  Kappa = c(log_metrics["Kappa"], tree_metrics["Kappa"], rf_metrics["Kappa"], svm_metrics["Kappa"])
)

# Printing metrics
print(metrics_df)

# Melt data frame for ggplot
metrics_melted <- metrics_df %>%
  pivot_longer(cols = -Model, names_to = "Metric", values_to = "Score")


# Ploting metrics
ggplot(metrics_melted, aes(x = Model, y = Score, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal() +
  labs(title = "Model Performance Metrics Comparison", y = "Score") +
  scale_fill_brewer(palette = "Set1")

# Ploting accuracy
ggplot(metrics_df, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Model Accuracy Comparison", y = "Accuracy")

# Ploting Kappa
ggplot(metrics_df, aes(x = Model, y = Kappa, fill = Model)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(title = "Model Kappa Comparison", y = "Kappa")


# Loading necessary libraries
library(caret)
library(randomForest)

# Setting up training control
ctrl <- trainControl(method = "cv", number = 5, savePredictions = "all")

# Defining the grid of hyperparameters for Random Forest
rf_grid <- expand.grid(mtry = c(2, 4, 6, 8, 10))  # Adjust these values as needed

# Set seed for reproducibility
set.seed(123)

# Performing grid search for Random Forest
tune_results_rf <- train(
  Nobeyesdad ~ .,               # Response variable
  data = train_data,            # Data frame
  method = "rf",               # Model method
  tuneGrid = rf_grid,          # Tuning grid
  trControl = ctrl,            # Train control
  importance = TRUE            # To compute feature importance
)

# Printing the results of the tuned model
print(tune_results_rf)

# Extracting feature importance
importance_df <- as.data.frame(importance(tune_results_rf$finalModel))

importance_df$Feature <- rownames(importance_df)


# Sorting by importance
importance_df <- importance_df %>% 
  arrange(desc(Overweight_Level_II))  


ggplot(importance_df, aes(x = reorder(Feature, Overweight_Level_II), y = Overweight_Level_II, fill = Overweight_Level_II)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Feature Importance from Random Forest") +
  xlab("Features") +
  ylab("Importance") +
  scale_fill_gradient(low = "blue", high = "red")




# Selecting top 10 important features
top_features <- importance_df %>%
  top_n(10, wt = Overweight_Level_II) %>%
  pull(Feature)

# Defining top features
top_features <- c('Weight', 'GenderMale', 'FCVC', 'Height', 'TUE','Age','CALCSometimes','family_history_with_overweightyes','CAECSometimes','MTRANSPublic_Transportation')

# Checking column names in train_data
existing_features <- colnames(train_data)

# Finding valid features
valid_features <- intersect(top_features, existing_features)

# Printing valid features
print(valid_features)

# Selecting valid features and target variable
train_data_reduced <- train_data %>%
  select(all_of(valid_features), Nobeyesdad)

# Printing the first few rows of the reduced dataset
head(train_data_reduced)

test_data_reduced <- test_data %>%
  select(all_of(valid_features), Nobeyesdad)

# Printing the first few rows of the reduced dataset
head(test_data_reduced)

# Re-training Random Forest with selected features
rf_model_reduced <- train(
  Nobeyesdad ~ ., 
  data = train_data_reduced, 
  method = "rf", 
  tuneGrid = rf_grid,
  trControl = ctrl,
  importance = TRUE
)

# Making predictions with the reduced feature model
rf_pred_reduced <- predict(rf_model_reduced, newdata = test_data_reduced)

# Evaluating the performance
rf_cm_reduced <- confusionMatrix(rf_pred_reduced, test_data_reduced$Nobeyesdad)
print(rf_cm_reduced)

# Performance of the model with all features
rf_cm_all <- confusionMatrix(rf_pred, test_data$Nobeyesdad)
print(rf_cm_all)

# Comparing the confusion matrices
print(rf_cm_all)
print(rf_cm_reduced)

library(pROC)
# Obtaining the predicted probabilities
rf_probs <- predict(rf_model, test_data, type = "prob")

#Ensuring the test data target variable is a factor
test_data$Nobeyesdad <- as.factor(test_data$Nobeyesdad)

#Calculating ROC curve and AUC for each class
# Initialize a list to store ROC curves
roc_list <- list()
auc_list <- c()

# Loop over each class to calculate the ROC and AUC
for (class_name in levels(test_data$Nobeyesdad)) {
  
  # Binary vector for the current class (one-vs-rest approach)
  binary_response <- ifelse(test_data$Nobeyesdad == class_name, 1, 0)
  
  # Calculating ROC curve
  roc_obj <- roc(binary_response, rf_probs[, class_name])
  
  # Storing the ROC curve
  roc_list[[class_name]] <- roc_obj
  
  # Calculating AUC
  auc_value <- auc(roc_obj)
  auc_list <- c(auc_list, auc_value)
  
  # Plotting the ROC curve
  plot(roc_obj, main = paste("ROC Curve for class:", class_name))
  cat("AUC for class", class_name, ": ", auc_value, "\n")
}

# Combining AUC values into a data frame
auc_results <- data.frame(Class = levels(test_data$Nobeyesdad), AUC = auc_list)

# Displaying the AUC results
cat("\nAUC Values for Each Class:\n")
print(auc_results)

